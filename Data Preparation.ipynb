{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d941bcb-5576-4307-aaa1-55cff41da7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d51f5b-27a3-4cf4-89f1-c6ff281106a6",
   "metadata": {},
   "source": [
    "### Loading train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38faf9fe-dc3e-437a-bdf8-37778020d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./train.txt\"\n",
    "test_data_path = \"./test.rand.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d3290-9466-4bc9-8f20-a8d78edf3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read(file_path):\n",
    "    valid_lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        for line in f:\n",
    "            if '\\ufffd' not in line:  # Check for replacement characters\n",
    "                valid_lines.append(line)\n",
    "    return valid_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff284a6f-7958-4536-bd47-a155215dc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = safe_read(train_data_path)\n",
    "train_data_df = pd.DataFrame([line.strip().split('\\t') for line in lines], columns=['Sentence_A', 'Sentence_B'])\n",
    "train_data_df['labels'] = 0\n",
    "train_data_df = train_data_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadd552-7903-4911-8efc-720b2f5495a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = safe_read(test_data_path)\n",
    "test_data_df = pd.DataFrame([line.strip().split('\\t') for line in lines], columns=['Sentence_A', \"Sentence_B\"])\n",
    "test_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3a656-2076-4ce5-b7c9-b9d161b255d3",
   "metadata": {},
   "source": [
    "### Tokenizing sentences into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522a93d-7b5b-4f6e-91cd-5c943cd14100",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def compute_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    embeddings_A = compute_embeddings(examples[\"Sentence_A\"])\n",
    "    embeddings_B = compute_embeddings(examples[\"Sentence_B\"])\n",
    "    return {\"embeddings_A\": embeddings_A.numpy(),\n",
    "            \"embeddings_B\": embeddings_B.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5a1e0-3801-4890-9e1d-fea1f618294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data_df.iloc[0:50000])\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset.save_to_disk(\"tokenized_train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb33acc-9094-4f83-94e7-e7300fcb9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_pandas(testdata)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset.save_to_disk(\"tokenized_test_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f84e3-c0ed-4515-bd99-206b4f8b4401",
   "metadata": {},
   "source": [
    "### Generating training samples with different labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b51754-1739-428b-93e2-a976e1f945e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_and_flip(dataset):\n",
    "    flipped_data = {\n",
    "        \"Sentence_A\": [],\n",
    "        \"Sentence_B\" : [],\n",
    "        \"labels\": [],\n",
    "        \"embeddings_A\": [],\n",
    "        \"embeddings_B\": []\n",
    "    }\n",
    "\n",
    "    for example in dataset:\n",
    "        # Swap embeddings and flip labels\n",
    "        flipped_data[\"Sentence_A\"].append(example[\"Sentence_B\"])\n",
    "        flipped_data[\"embeddings_A\"].append(np.array(example[\"embeddings_B\"], dtype=np.float32))\n",
    "        flipped_data[\"Sentence_B\"].append(example[\"Sentence_A\"])\n",
    "        flipped_data[\"embeddings_B\"].append(np.array(example[\"embeddings_A\"], dtype=np.float32))\n",
    "        flipped_data[\"labels\"].append(1 - example[\"labels\"])  # Flip label (0 -> 1, 1 -> 0)\n",
    "\n",
    "    flipped_dataset = Dataset.from_dict(flipped_data)\n",
    "    return flipped_dataset\n",
    "\n",
    "flipped_train_dataset = duplicate_and_flip(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d63f0-6ef5-4ae7-b477-ec915fe75a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_data = concatenate_datasets([tokenized_train_dataset, flipped_train_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532e669-33e8-4ae8-af9f-d58780993080",
   "metadata": {},
   "source": [
    "### Computing difference in embeddings of Sentences for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaeda9b-8cd9-4ad0-a000-3e839df10677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the difference of embeddings\n",
    "def compute_difference(examples):\n",
    "    embeddings_A = torch.tensor(examples[\"embeddings_A\"])\n",
    "    embeddings_B = torch.tensor(examples[\"embeddings_B\"])\n",
    "    difference = embeddings_A - embeddings_B\n",
    "    return {\"difference\": difference.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c156812-6d9c-459a-992c-db1bbbedbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_dataset = concat_train_data.map(compute_difference, batched=True)\n",
    "final_train_dataset.save_to_disk(\"final_train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06974abf-829f-4c8c-a90c-10c9a93d23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_dataset = tokenized_test_dataset.map(compute_difference, batched=True)\n",
    "final_test_dataset.save_to_disk(\"final_test_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
